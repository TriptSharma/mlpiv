{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainerMLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TriptSharma/mlpiv/blob/master/TrainerMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16fBwZ322XeW",
        "colab_type": "code",
        "outputId": "c03802c0-2618-4f43-b177-7b5a7f639f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1047
        }
      },
      "source": [
        "!pip install scikit-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7fe7b0558c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install scikit-learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRp4Lq9z9JGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xcc8r16A-0v",
        "colab_type": "code",
        "outputId": "8f42d7d9-a9c9-4570-a204-f333d03760d5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5af2cfdc-142e-4650-bf97-4c40e5d3f38a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5af2cfdc-142e-4650-bf97-4c40e5d3f38a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxradEatBJ4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['data.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlnWDQ5wBUV_",
        "colab_type": "code",
        "outputId": "4fd8f4cd-32fa-42fb-e98b-a04c033aa533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1143
        }
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0       0     1     2             3         4         5\n",
            "0              0   0.005  0.09  0.09 -1.470000e-09 -0.000005  0.000005\n",
            "1              1   0.010  0.09  0.09  1.230000e-09 -0.000011  0.000011\n",
            "2              2   0.015  0.09  0.09  8.630000e-10 -0.000016  0.000016\n",
            "3              3   0.020  0.09  0.09  1.070000e-09 -0.000021  0.000021\n",
            "4              4   0.025  0.09  0.09  1.310000e-09 -0.000027  0.000027\n",
            "5              5   0.030  0.09  0.09  7.240000e-10 -0.000032  0.000032\n",
            "6              6   0.035  0.09  0.09  8.620000e-10 -0.000037  0.000037\n",
            "7              7   0.040  0.09  0.09  1.020000e-09 -0.000042  0.000042\n",
            "8              8   0.045  0.09  0.09  1.180000e-09 -0.000047  0.000047\n",
            "9              9   0.050  0.09  0.09  1.350000e-09 -0.000052  0.000052\n",
            "10            10   0.055  0.09  0.09  1.530000e-09 -0.000056  0.000056\n",
            "11            11   0.060  0.09  0.09  1.710000e-09 -0.000061  0.000061\n",
            "12            12   0.065  0.09  0.09  8.740000e-10 -0.000066  0.000066\n",
            "13            13   0.070  0.09  0.09  9.590000e-10 -0.000070  0.000070\n",
            "14            14   0.075  0.09  0.09  1.060000e-09 -0.000075  0.000075\n",
            "15            15   0.080  0.09  0.09  1.140000e-09 -0.000080  0.000080\n",
            "16            16   0.085  0.09  0.09  1.230000e-09 -0.000084  0.000084\n",
            "17            17   0.090  0.09  0.09  1.310000e-09 -0.000088  0.000088\n",
            "18            18   0.095  0.09  0.09  1.400000e-09 -0.000093  0.000093\n",
            "19            19   0.100  0.09  0.09  1.480000e-09 -0.000097  0.000097\n",
            "20            20   0.105  0.09  0.09  1.490000e-09 -0.000101  0.000101\n",
            "21            21   0.110  0.09  0.09  1.370000e-09 -0.000105  0.000105\n",
            "22            22   0.115  0.09  0.09  1.410000e-09 -0.000109  0.000109\n",
            "23            23   0.120  0.09  0.09  1.660000e-09 -0.000113  0.000113\n",
            "24            24   0.125  0.09  0.09  1.730000e-09 -0.000117  0.000117\n",
            "25            25   0.130  0.09  0.09  1.790000e-09 -0.000121  0.000121\n",
            "26            26   0.135  0.09  0.09  1.850000e-09 -0.000125  0.000125\n",
            "27            27   0.140  0.09  0.09  1.930000e-09 -0.000129  0.000128\n",
            "28            28   0.145  0.09  0.09  1.970000e-09 -0.000132  0.000132\n",
            "29            29   0.150  0.09  0.09  2.020000e-09 -0.000136  0.000136\n",
            "...          ...     ...   ...   ...           ...       ...       ...\n",
            "1970        1970   9.855  0.09  0.09  1.620000e-08  0.002159 -0.004518\n",
            "1971        1971   9.860  0.09  0.09  1.610000e-08  0.002159 -0.004519\n",
            "1972        1972   9.865  0.09  0.09  1.600000e-08  0.002160 -0.004520\n",
            "1973        1973   9.870  0.09  0.09  1.590000e-08  0.002160 -0.004520\n",
            "1974        1974   9.875  0.09  0.09  1.570000e-08  0.002160 -0.004521\n",
            "1975        1975   9.880  0.09  0.09  1.570000e-08  0.002160 -0.004522\n",
            "1976        1976   9.885  0.09  0.09  1.560000e-08  0.002160 -0.004522\n",
            "1977        1977   9.890  0.09  0.09  1.570000e-08  0.002160 -0.004523\n",
            "1978        1978   9.895  0.09  0.09  1.580000e-08  0.002161 -0.004523\n",
            "1979        1979   9.900  0.09  0.09  1.590000e-08  0.002161 -0.004524\n",
            "1980        1980   9.905  0.09  0.09  1.610000e-08  0.002161 -0.004525\n",
            "1981        1981   9.910  0.09  0.09  1.620000e-08  0.002161 -0.004525\n",
            "1982        1982   9.915  0.09  0.09  1.630000e-08  0.002161 -0.004526\n",
            "1983        1983   9.920  0.09  0.09  1.640000e-08  0.002161 -0.004526\n",
            "1984        1984   9.925  0.09  0.09  1.650000e-08  0.002161 -0.004527\n",
            "1985        1985   9.930  0.09  0.09  1.650000e-08  0.002162 -0.004528\n",
            "1986        1986   9.935  0.09  0.09  1.650000e-08  0.002162 -0.004528\n",
            "1987        1987   9.940  0.09  0.09  1.650000e-08  0.002162 -0.004529\n",
            "1988        1988   9.945  0.09  0.09  1.640000e-08  0.002162 -0.004529\n",
            "1989        1989   9.950  0.09  0.09  4.400000e-08  0.002162 -0.004530\n",
            "1990        1990   9.955  0.09  0.09  1.450000e-08  0.002162 -0.004531\n",
            "1991        1991   9.960  0.09  0.09  4.170000e-08  0.002162 -0.004531\n",
            "1992        1992   9.965  0.09  0.09  1.430000e-08  0.002163 -0.004532\n",
            "1993        1993   9.970  0.09  0.09  4.140000e-08  0.002163 -0.004532\n",
            "1994        1994   9.975  0.09  0.09  3.800000e-08  0.002163 -0.004533\n",
            "1995        1995   9.980  0.09  0.09  3.670000e-08  0.002163 -0.004533\n",
            "1996        1996   9.985  0.09  0.09  3.710000e-08  0.002163 -0.004534\n",
            "1997        1997   9.990  0.09  0.09  3.750000e-08  0.002163 -0.004535\n",
            "1998        1998   9.995  0.09  0.09  3.780000e-08  0.002163 -0.004535\n",
            "1999        1999  10.000  0.09  0.09  3.820000e-08  0.002164 -0.004536\n",
            "\n",
            "[2000 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDNmwQJi9GIL",
        "colab_type": "code",
        "outputId": "ae07462e-e29f-40ab-d0a6-ed00601558e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "df = pd.read_csv(r'data.csv')\n",
        "\n",
        "X = df.drop('3', axis=1)\n",
        "y = df['3']\n",
        "print(X.head(), y.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0      0     1     2         4         5\n",
            "0           0  0.005  0.09  0.09 -0.000005  0.000005\n",
            "1           1  0.010  0.09  0.09 -0.000011  0.000011\n",
            "2           2  0.015  0.09  0.09 -0.000016  0.000016\n",
            "3           3  0.020  0.09  0.09 -0.000021  0.000021\n",
            "4           4  0.025  0.09  0.09 -0.000027  0.000027 0   -1.470000e-09\n",
            "1    1.230000e-09\n",
            "2    8.630000e-10\n",
            "3    1.070000e-09\n",
            "4    1.310000e-09\n",
            "Name: 3, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJERZaBN_skO",
        "colab_type": "code",
        "outputId": "f347e3cd-aac7-41bf-c2a4-b1d02fb33ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "X_scaled = preprocessing.scale(X)\n",
        "y_scaled = preprocessing.scale(y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ1t0o1AEtXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test = X_scaled[:1600], X_scaled[1600:]\n",
        "y_train, y_test = y_scaled[:1600], y_scaled[1600:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAZ60fg_58I",
        "colab_type": "code",
        "outputId": "d5328b7c-c0f8-4ecf-b4c1-4876351ae54f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print(X_scaled)\n",
        "print(y_scaled)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.731185   -1.731185    0.          0.         -1.12349871  1.12740874]\n",
            " [-1.72945295 -1.72945295  0.          0.         -1.12880319  1.13028994]\n",
            " [-1.7277209  -1.7277209   0.          0.         -1.13404674  1.13313927]\n",
            " ...\n",
            " [ 1.7277209   1.7277209   0.          0.          1.0180526  -1.30952919]\n",
            " [ 1.72945295  1.72945295  0.          0.          1.01818098 -1.30981904]\n",
            " [ 1.731185    1.731185    0.          0.          1.01830935 -1.31011426]]\n",
            "[-1.46857234 -1.31588783 -1.33664162 ...  0.73517407  0.75213902\n",
            "  0.77475895]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sNkb5y3RrKv",
        "colab_type": "code",
        "outputId": "60357c1c-fe9f-4413-b1dd-cc1c8039fa40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "y_predicted= mlp.predict(X_test)\n",
        "print(\"Mean squared error %f\" % mean_squared_error(y_test,y_predicted))\n",
        "print(\"Variance score %f\" % r2_score(y_test,y_predicted))\n",
        "print(\"Mean absolute error %f\" % mean_absolute_error(y_test,y_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.889180\n",
            "Test set score: -1.210550\n",
            "Mean squared error 2.580799\n",
            "Variance score -1.210550\n",
            "Mean absolute error 1.319493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIKGpu5GXgHP",
        "colab_type": "code",
        "outputId": "2cc7db25-9e55-4f69-bccb-e713275b6726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2463
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=5, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08,n_iter_no_change=10)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "y_predicted= mlp.predict(X_test)\n",
        "print(\"Mean squared error %f\" % mean_squared_error(y_test,y_predicted))\n",
        "print(\"Variance score %f\" % r2_score(y_test,y_predicted))\n",
        "print(\"Mean absolute error %f\" % mean_absolute_error(y_test,y_predicted))\n",
        "print(y_predicted)\n",
        "print(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.898962\n",
            "Test set score: -0.727117\n",
            "Mean squared error 2.016395\n",
            "Variance score -0.727117\n",
            "Mean absolute error 1.221839\n",
            "[0.50398842 0.50367227 0.50323511 0.50279795 0.50236079 0.50189963\n",
            " 0.50146246 0.50088029 0.50029812 0.49983696 0.49925479 0.49896264\n",
            " 0.49838047 0.49777429 0.49716812 0.49656195 0.49595578 0.4953256\n",
            " 0.49469543 0.49406526 0.49343509 0.49278091 0.49222375 0.49156957\n",
            " 0.4909884  0.49043124 0.48982607 0.4892449  0.48873674 0.48813157\n",
            " 0.48772042 0.48718826 0.48675311 0.48631795 0.4858588  0.48549665\n",
            " 0.48511051 0.48484537 0.48455623 0.4843641  0.48414797 0.48398426\n",
            " 0.4838075  0.48370363 0.48357565 0.48354467 0.48348959 0.4835315\n",
            " 0.48367042 0.48378522 0.48399703 0.48416063 0.48442122 0.48480292\n",
            " 0.48525751 0.4856639  0.48619138 0.48667066 0.48736804 0.48804131\n",
            " 0.48869048 0.48921912 0.49032859 0.49093013 0.49179798 0.49285982\n",
            " 0.49389756 0.4950323  0.49625113 0.4975708  0.49884211 0.50050131\n",
            " 0.5020638  0.50345671 0.50480128 0.50648537 0.50814529 0.5100235\n",
            " 0.51185336 0.5137561  0.51575591 0.5178286  0.51997418 0.52209558\n",
            " 0.5244111  0.52670245 0.52906668 0.53152797 0.53406215 0.53654797\n",
            " 0.53925209 0.54193204 0.54468487 0.54751059 0.55031213 0.55333197\n",
            " 0.55630346 0.55934783 0.5623922  0.56560652 0.56882083 0.5719868\n",
            " 0.57537106 0.57870698 0.58204289 0.58554875 0.5890546  0.59251211\n",
            " 0.59618791 0.59983954 0.603467   0.60728857 0.61098891 0.61488336\n",
            " 0.61875365 0.62262393 0.6265671  0.63048609 0.63452632 0.6385182\n",
            " 0.64251008 0.64659902 0.65066378 0.65472854 0.65874496 0.66288261\n",
            " 0.66689902 0.6710125  0.67512597 0.67921527 0.6832804  0.6873697\n",
            " 0.69143483 0.69547578 0.69941967 0.70346062 0.70750158 0.71142129\n",
            " 0.71543807 0.71935779 0.72325333 0.72714888 0.73106859 0.7348429\n",
            " 0.73861721 0.74239152 0.74619001 0.74996432 0.75373863 0.75741588\n",
            " 0.76106895 0.76472203 0.7683751  0.77190695 0.7755842  0.77914021\n",
            " 0.78279329 0.78637348 0.7899295  0.79336428 0.79694447 0.80025802\n",
            " 0.80371697 0.8070547  0.81039242 0.81375432 0.81711622 0.82035688\n",
            " 0.82350048 0.82676532 0.82990893 0.8330767  0.83612325 0.83916979\n",
            " 0.84222017 0.84517698 0.84825502 0.85123601 0.85409577 0.85695552\n",
            " 0.85983946 0.86269921 0.86558315 0.86837003 0.87113274 0.87391962\n",
            " 0.87658527 0.87939634 0.88206199 0.88475182 0.88744165 0.89013148\n",
            " 0.89270008 0.89541409 0.89798269 0.90057547 0.90304701 0.90566397\n",
            " 0.90813551 0.91063124 0.91312696 0.91562269 0.9181426  0.92054127\n",
            " 0.92293995 0.92533862 0.92773729 0.93001473 0.93243758 0.9347392\n",
            " 0.93704082 0.93934244 0.94152283 0.94384863 0.94602901 0.94823358\n",
            " 0.95043815 0.95264271 0.95482309 0.95705184 0.95925641 0.96146098\n",
            " 0.96354431 0.96574887 0.9678322  0.97006095 0.97214428 0.97422761\n",
            " 0.97633512 0.97841845 0.98050178 0.98260929 0.98469262 0.98680014\n",
            " 0.98876223 0.99086974 0.99283183 0.99493934 0.99692562 0.99888771\n",
            " 1.00087399 1.00286026 1.00484654 1.00683281 1.00881909 1.01068412\n",
            " 1.0126704  1.01453544 1.01652171 1.01838675 1.0202276  1.02209264\n",
            " 1.02395768 1.02582271 1.02780899 1.02967403 1.03153907 1.0334041\n",
            " 1.0351479  1.03701294 1.03887798 1.04074301 1.04260805 1.0444489\n",
            " 1.0461927  1.04805774 1.04989859 1.05164239 1.05350743 1.05522704\n",
            " 1.05709208 1.05883588 1.06067673 1.06242053 1.06426139 1.06600518\n",
            " 1.0677248  1.0694686  1.07130945 1.07305325 1.07477287 1.07651666\n",
            " 1.07835752 1.08007713 1.08179675 1.08351636 1.08523597 1.08722225\n",
            " 1.08896605 1.09066148 1.09223567 1.09407652 1.09577195 1.09761281\n",
            " 1.09930823 1.1011249  1.10282033 1.104637   1.10645367 1.10827033\n",
            " 1.11006282 1.11173406 1.11355073 1.11534321 1.11725693 1.11902523\n",
            " 1.12081771 1.1226102  1.12449973 1.12626803 1.12815756 1.12992586\n",
            " 1.1318154  1.13368075 1.1355461  1.13741145 1.1392768  1.14114215\n",
            " 1.14298332 1.14494572 1.14678689 1.14874929 1.15068751 1.15264992\n",
            " 1.15458813 1.15650217 1.15844039 1.16035442 1.16238969 1.16442497\n",
            " 1.16643605 1.16844714 1.17045823 1.17244513 1.17445349 1.17655377\n",
            " 1.17862991 1.1805849  1.18266103 1.18485831 1.1869103  1.18896228\n",
            " 1.19113541 1.19328439 1.19543338 1.19758236 1.19970719 1.20183203\n",
            " 1.204078   1.20617869 1.20840052 1.21062235 1.21282003 1.21513886\n",
            " 1.21733654 1.21963121 1.22192589 1.22407528 1.2264911  1.22873748\n",
            " 1.23100801 1.23337554 1.23571891 1.23808644 1.24040567 1.24274905\n",
            " 1.24518942 1.24750865 1.24992487 1.2523411  1.25473318 1.25724639\n",
            " 1.25963847 1.26215169 1.26449548 1.26698455 1.26944948 1.27203554\n",
            " 1.27447632 1.27703824 1.27947902 1.28201679 1.28455457 1.28709234\n",
            " 1.28972711 1.29224074 1.29485136 1.29734084 1.29992731 1.30268322\n",
            " 1.30510026 1.30795316 1.31037021 1.31319896 1.31573714 1.31839646\n",
            " 1.32103164 1.32369096 1.32630199 1.32893717]\n",
            "[ 0.30539546  0.31105044  0.31670542  0.3223604   0.3223604   0.31670542\n",
            "  0.31105044  0.29408549  0.27712055  0.2601556   0.23188069  2.99151182\n",
            "  3.20074615  3.04240666  2.9688919   2.94061699  2.94061699  2.96323691\n",
            "  2.99151182  3.0310967   3.07068157  3.10461146  3.12723139  3.14419633\n",
            "  3.1555063   3.14985131  3.13288637  3.10461146  3.06502659  3.00847677\n",
            "  2.934962    2.83882731  2.73703764  2.62959299  2.55042324  2.51649335\n",
            "  2.54476826  2.64090295  2.73138266  2.81620739  2.88406717  2.94061699\n",
            "  2.97454688  2.9971668   3.00282179  2.98585684  2.95192695  2.89537713\n",
            "  2.81620739  2.71441771  2.58435313  2.45428855  2.35249888  2.32987895\n",
            "  2.38642877  2.48256346  2.58435313  2.66917786  2.74269262  2.80489742\n",
            "  2.85579226  0.2601556   2.53911328  0.32801538  0.17533087  0.22057073\n",
            "  0.23188069  0.22622571  0.21491575  0.1979508   0.16402091  2.45428855\n",
            "  2.61828302  2.34684389  0.08485116  0.04526629  0.17533087  0.24319066\n",
            "  0.29408549  0.33367036  0.36194527  0.3845652   0.39022018  0.3845652\n",
            "  0.37325524  0.34498033  0.30539546  0.24884564  0.17533087  0.07919618\n",
            "  0.03961131  0.0735412   0.11878106  0.15836593  0.18664084  0.21491575\n",
            "  0.23753567  0.2601556   0.27712055  0.29408549  0.30539546  0.3223604\n",
            "  0.33367036  0.34498033  0.35629029  0.37325524  0.37891022  0.37891022\n",
            "  0.36194527  0.3223604   0.23753567  0.20926076  0.29974047  0.37891022\n",
            "  0.44677     0.49766484  0.53159473  0.55421466  0.55986964  0.55421466\n",
            "  0.53159473  0.50331982  0.47504491  0.42415007  0.35629029  0.2601556\n",
            "  0.15271095  0.15836593  0.19229582  0.21491575  0.24319066  0.26581058\n",
            "  0.28277553  0.29974047  0.31105044  0.31670542  0.3223604   0.32801538\n",
            "  0.33367036  0.34498033  0.35629029  0.36194527  0.36194527  0.36760026\n",
            "  0.36760026  0.35629029  0.31105044  0.23753567  0.28277553  0.36194527\n",
            "  0.41284011  0.45242498 -0.46368207 -0.50892193 -0.48064702 -0.486302\n",
            " -0.49761196  0.34498033  0.44111502  0.35063531  0.27712055  0.20926076\n",
            "  0.10747109  0.09050615  0.10747109  0.13009102  0.14705596  0.16402091\n",
            "  0.17533087  0.18664084  0.19229582  0.1979508   0.1979508   0.1979508\n",
            "  0.1979508   0.1979508   0.1979508   0.19229582  0.19229582  0.18664084\n",
            "  0.17533087  0.15271095  0.10181611  0.06788622  0.11312607  0.15836593\n",
            "  0.18664084  0.1979508   0.20360578  0.20360578  0.1979508   0.18664084\n",
            "  0.16967589  0.14705596  0.12443604  0.08485116  0.03961131 -0.01693851\n",
            " -0.10176324 -0.11872818 -0.10176324 -0.09045327 -0.07348833 -0.06217836\n",
            " -0.0508684  -0.04521342 -0.04521342 -0.04521342 -0.04521342 -0.0508684\n",
            " -0.05652338 -0.06217836 -0.07348833 -0.08479829 -0.09610825 -0.1130732\n",
            " -0.14134811 -0.18658796 -0.21486287 -0.18093298 -0.14134811 -0.12438316\n",
            " -0.1130732  -0.10176324 -0.10176324 -0.10176324 -0.1130732  -0.12438316\n",
            " -0.14134811 -0.15831305 -0.18093298 -0.21486287 -0.26575771 -0.33927247\n",
            " -0.39582229 -0.37885735 -0.3618924  -0.34492745 -0.33927247 -0.32796251\n",
            " -0.32230753 -0.32230753 -0.32230753 -0.32230753 -0.32796251 -0.33361749\n",
            " -0.34492745 -0.35623742 -0.37320236 -0.39016731 -0.41844222 -0.45237211\n",
            " -0.49195698 -0.50326695 -0.48064702 -0.45237211 -0.4240972  -0.41278724\n",
            " -0.40147727 -0.39016731 -0.39016731 -0.39582229 -0.40147727 -0.41278724\n",
            " -0.42975218 -0.44671713 -0.47499204 -0.51457691 -0.55416178 -0.59374665\n",
            " -0.60505662 -0.58809167 -0.57112673 -0.55981676 -0.55416178 -0.54285182\n",
            " -0.54285182 -0.53719684 -0.53719684 -0.54285182 -0.5485068  -0.55416178\n",
            " -0.56547175 -0.57678171 -0.59374665 -0.61636658 -0.63898651  0.53724971\n",
            "  0.71820913  0.70689917 -0.58809167 -0.63898651 -0.59374665 -0.57678171\n",
            " -0.57112673 -0.56547175 -0.55981676 -0.55981676 -0.56547175 -0.57112673\n",
            " -0.58243669 -0.59374665 -0.61636658 -0.64464149 -0.66726142 -0.67857138\n",
            " -0.6729164  -0.65595145 -0.64464149 -0.63333153 -0.61636658 -0.60505662\n",
            " -0.59940164 -0.59940164 -0.59940164 -0.59940164 -0.59940164 -0.60505662\n",
            " -0.6107116  -0.62202156 -0.63333153 -0.65029647 -0.65595145 -0.66160644\n",
            " -0.65029647 -0.62767655 -0.6107116  -0.59374665 -0.57678171 -0.56547175\n",
            " -0.55416178 -0.5485068  -0.54285182 -0.54285182 -0.54285182 -0.5485068\n",
            " -0.55981676 -0.57112673 -0.58243669 -0.59374665 -0.60505662 -0.59940164\n",
            " -0.59374665 -0.58243669 -0.57112673 -0.55981676 -0.55416178 -0.54285182\n",
            " -0.53719684 -0.53154185 -0.53154185 -0.53154185 -0.53154185 -0.53719684\n",
            " -0.54285182 -0.5485068  -0.55416178 -0.55981676 -0.55981676 -0.55416178\n",
            " -0.54285182 -0.53154185 -0.51457691 -0.50326695 -0.49195698 -0.48064702\n",
            " -0.47499204 -0.46933705 -0.46368207 -0.46368207 -0.46933705 -0.47499204\n",
            " -0.48064702 -0.486302   -0.49761196 -0.49761196 -0.50326695 -0.49761196\n",
            " -0.49195698 -0.486302   -0.47499204 -0.46933705 -0.46368207 -0.45802709\n",
            " -0.45237211 -0.45237211 -0.45237211 -0.45237211 -0.45802709  1.10274789\n",
            " -0.56547175  0.97268331 -0.57678171  0.95571837  0.76344898  0.68993422\n",
            "  0.71255415  0.73517407  0.75213902  0.77475895]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwuESb8lWXpv",
        "colab_type": "code",
        "outputId": "30dcb450-557f-4393-c059-8d5e563e0a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1878
        }
      },
      "source": [
        "\n",
        "+mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='adam', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=5, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "y_predicted= mlp.predict(X_test)\n",
        "print(\"Mean squared error %f\" % mean_squared_error(y_test,y_predicted))\n",
        "print(\"Variance score %f\" % r2_score(y_test,y_predicted))\n",
        "print(\"Mean absolute error %f\" % mean_absolute_error(y_test,y_predicted))\n",
        "pd.DataFrame(mlp.loss_curve_).plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.36209428\n",
            "Iteration 2, loss = 0.33144192\n",
            "Iteration 3, loss = 0.32260318\n",
            "Iteration 4, loss = 0.32274582\n",
            "Iteration 5, loss = 0.31177503\n",
            "Iteration 6, loss = 0.30838490\n",
            "Iteration 7, loss = 0.30519217\n",
            "Iteration 8, loss = 0.31073464\n",
            "Iteration 9, loss = 0.30954285\n",
            "Iteration 10, loss = 0.30136671\n",
            "Iteration 11, loss = 0.29862087\n",
            "Iteration 12, loss = 0.29341929\n",
            "Iteration 13, loss = 0.29254029\n",
            "Iteration 14, loss = 0.28346808\n",
            "Iteration 15, loss = 0.28182133\n",
            "Iteration 16, loss = 0.28580707\n",
            "Iteration 17, loss = 0.27775341\n",
            "Iteration 18, loss = 0.27007386\n",
            "Iteration 19, loss = 0.26260211\n",
            "Iteration 20, loss = 0.25802480\n",
            "Iteration 21, loss = 0.26679958\n",
            "Iteration 22, loss = 0.28432257\n",
            "Iteration 23, loss = 0.26555328\n",
            "Iteration 24, loss = 0.25324839\n",
            "Iteration 25, loss = 0.25177425\n",
            "Iteration 26, loss = 0.26343488\n",
            "Iteration 27, loss = 0.24538199\n",
            "Iteration 28, loss = 0.23208688\n",
            "Iteration 29, loss = 0.22683810\n",
            "Iteration 30, loss = 0.22886803\n",
            "Iteration 31, loss = 0.21907180\n",
            "Iteration 32, loss = 0.21653426\n",
            "Iteration 33, loss = 0.21114409\n",
            "Iteration 34, loss = 0.21186286\n",
            "Iteration 35, loss = 0.20797218\n",
            "Iteration 36, loss = 0.20183258\n",
            "Iteration 37, loss = 0.19570963\n",
            "Iteration 38, loss = 0.19351042\n",
            "Iteration 39, loss = 0.19278817\n",
            "Iteration 40, loss = 0.18975913\n",
            "Iteration 41, loss = 0.18337444\n",
            "Iteration 42, loss = 0.18349048\n",
            "Iteration 43, loss = 0.18356861\n",
            "Iteration 44, loss = 0.17426823\n",
            "Iteration 45, loss = 0.17291776\n",
            "Iteration 46, loss = 0.16648317\n",
            "Iteration 47, loss = 0.16289410\n",
            "Iteration 48, loss = 0.15598521\n",
            "Iteration 49, loss = 0.15293310\n",
            "Iteration 50, loss = 0.15641732\n",
            "Iteration 51, loss = 0.15888110\n",
            "Iteration 52, loss = 0.15404715\n",
            "Iteration 53, loss = 0.15443892\n",
            "Iteration 54, loss = 0.15246069\n",
            "Iteration 55, loss = 0.14997700\n",
            "Iteration 56, loss = 0.14856275\n",
            "Iteration 57, loss = 0.15129246\n",
            "Iteration 58, loss = 0.14124994\n",
            "Iteration 59, loss = 0.15986763\n",
            "Iteration 60, loss = 0.14145296\n",
            "Iteration 61, loss = 0.13891288\n",
            "Iteration 62, loss = 0.13515508\n",
            "Iteration 63, loss = 0.13911455\n",
            "Iteration 64, loss = 0.13639294\n",
            "Iteration 65, loss = 0.13199657\n",
            "Iteration 66, loss = 0.12997944\n",
            "Iteration 67, loss = 0.12934390\n",
            "Iteration 68, loss = 0.12556827\n",
            "Iteration 69, loss = 0.12261578\n",
            "Iteration 70, loss = 0.13116990\n",
            "Iteration 71, loss = 0.12767813\n",
            "Iteration 72, loss = 0.12889958\n",
            "Iteration 73, loss = 0.12986214\n",
            "Iteration 74, loss = 0.13252550\n",
            "Iteration 75, loss = 0.12673427\n",
            "Iteration 76, loss = 0.12302379\n",
            "Iteration 77, loss = 0.12905644\n",
            "Iteration 78, loss = 0.13968786\n",
            "Iteration 79, loss = 0.12425563\n",
            "Iteration 80, loss = 0.12596496\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.756563\n",
            "Test set score: -1.891562\n",
            "Mean squared error 3.375876\n",
            "Variance score -1.891562\n",
            "Mean absolute error 1.616831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f86c7756b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioVGu1ccS9z3",
        "colab_type": "code",
        "outputId": "9f77ff65-f43f-4799-d05b-9f0f29282f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8553
        }
      },
      "source": [
        "opts = ['adam','sgd']\n",
        "for i in opts:\n",
        "  mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver=i, \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=5, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "  \n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "y_predicted= mlp.predict(X_test)\n",
        "print(\"Mean squared error %f\" % mean_squared_error(y_test,y_predicted))\n",
        "print(\"Variance score %f\" % r2_score(y_test,y_predicted))\n",
        "print(\"Mean absolute error %f\" % mean_absolute_error(y_test,y_predicted))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.36808423\n",
            "Iteration 2, loss = 0.33757173\n",
            "Iteration 3, loss = 0.33072778\n",
            "Iteration 4, loss = 0.32555517\n",
            "Iteration 5, loss = 0.32222735\n",
            "Iteration 6, loss = 0.31973666\n",
            "Iteration 7, loss = 0.31876505\n",
            "Iteration 8, loss = 0.31701523\n",
            "Iteration 9, loss = 0.31538058\n",
            "Iteration 10, loss = 0.31692836\n",
            "Iteration 11, loss = 0.31334388\n",
            "Iteration 12, loss = 0.31365099\n",
            "Iteration 13, loss = 0.31283829\n",
            "Iteration 14, loss = 0.31193084\n",
            "Iteration 15, loss = 0.31282681\n",
            "Iteration 16, loss = 0.31069608\n",
            "Iteration 17, loss = 0.31043788\n",
            "Iteration 18, loss = 0.31155121\n",
            "Iteration 19, loss = 0.30924990\n",
            "Iteration 20, loss = 0.30930683\n",
            "Iteration 21, loss = 0.31257241\n",
            "Iteration 22, loss = 0.30920690\n",
            "Iteration 23, loss = 0.30801356\n",
            "Iteration 24, loss = 0.30812312\n",
            "Iteration 25, loss = 0.31020225\n",
            "Iteration 26, loss = 0.30901348\n",
            "Iteration 27, loss = 0.30696233\n",
            "Iteration 28, loss = 0.30613749\n",
            "Iteration 29, loss = 0.30521245\n",
            "Iteration 30, loss = 0.30732364\n",
            "Iteration 31, loss = 0.30548600\n",
            "Iteration 32, loss = 0.30475871\n",
            "Iteration 33, loss = 0.30548123\n",
            "Iteration 34, loss = 0.30521980\n",
            "Iteration 35, loss = 0.30412588\n",
            "Iteration 36, loss = 0.30361592\n",
            "Iteration 37, loss = 0.30326284\n",
            "Iteration 38, loss = 0.30268299\n",
            "Iteration 39, loss = 0.30270697\n",
            "Iteration 40, loss = 0.30248332\n",
            "Iteration 41, loss = 0.30277591\n",
            "Iteration 42, loss = 0.30278698\n",
            "Iteration 43, loss = 0.30300311\n",
            "Iteration 44, loss = 0.30063087\n",
            "Iteration 45, loss = 0.30069291\n",
            "Iteration 46, loss = 0.30043291\n",
            "Iteration 47, loss = 0.29973057\n",
            "Iteration 48, loss = 0.29964913\n",
            "Iteration 49, loss = 0.29970754\n",
            "Iteration 50, loss = 0.29903822\n",
            "Iteration 51, loss = 0.29945548\n",
            "Iteration 52, loss = 0.30057975\n",
            "Iteration 53, loss = 0.29883944\n",
            "Iteration 54, loss = 0.29789573\n",
            "Iteration 55, loss = 0.29768110\n",
            "Iteration 56, loss = 0.29703576\n",
            "Iteration 57, loss = 0.29718967\n",
            "Iteration 58, loss = 0.29774875\n",
            "Iteration 59, loss = 0.29722271\n",
            "Iteration 60, loss = 0.29610105\n",
            "Iteration 61, loss = 0.29540350\n",
            "Iteration 62, loss = 0.29494171\n",
            "Iteration 63, loss = 0.29543225\n",
            "Iteration 64, loss = 0.29341833\n",
            "Iteration 65, loss = 0.29578897\n",
            "Iteration 66, loss = 0.29417333\n",
            "Iteration 67, loss = 0.29298317\n",
            "Iteration 68, loss = 0.29226718\n",
            "Iteration 69, loss = 0.29399579\n",
            "Iteration 70, loss = 0.29278413\n",
            "Iteration 71, loss = 0.29197915\n",
            "Iteration 72, loss = 0.29198757\n",
            "Iteration 73, loss = 0.29021340\n",
            "Iteration 74, loss = 0.29132658\n",
            "Iteration 75, loss = 0.29134393\n",
            "Iteration 76, loss = 0.28951074\n",
            "Iteration 77, loss = 0.29008612\n",
            "Iteration 78, loss = 0.29367117\n",
            "Iteration 79, loss = 0.28967168\n",
            "Iteration 80, loss = 0.28907246\n",
            "Iteration 81, loss = 0.28790161\n",
            "Iteration 82, loss = 0.28697100\n",
            "Iteration 83, loss = 0.28778299\n",
            "Iteration 84, loss = 0.28659294\n",
            "Iteration 85, loss = 0.28600701\n",
            "Iteration 86, loss = 0.28559593\n",
            "Iteration 87, loss = 0.28496150\n",
            "Iteration 88, loss = 0.28374376\n",
            "Iteration 89, loss = 0.28356714\n",
            "Iteration 90, loss = 0.28407596\n",
            "Iteration 91, loss = 0.28229803\n",
            "Iteration 92, loss = 0.28212230\n",
            "Iteration 93, loss = 0.28170061\n",
            "Iteration 94, loss = 0.28181750\n",
            "Iteration 95, loss = 0.28086828\n",
            "Iteration 96, loss = 0.27967574\n",
            "Iteration 97, loss = 0.27931560\n",
            "Iteration 98, loss = 0.27896613\n",
            "Iteration 99, loss = 0.27887395\n",
            "Iteration 100, loss = 0.27939207\n",
            "Iteration 101, loss = 0.27812813\n",
            "Iteration 102, loss = 0.27670507\n",
            "Iteration 103, loss = 0.27651096\n",
            "Iteration 104, loss = 0.27493969\n",
            "Iteration 105, loss = 0.27606860\n",
            "Iteration 106, loss = 0.27521989\n",
            "Iteration 107, loss = 0.27394687\n",
            "Iteration 108, loss = 0.27491258\n",
            "Iteration 109, loss = 0.27527417\n",
            "Iteration 110, loss = 0.27196625\n",
            "Iteration 111, loss = 0.27124289\n",
            "Iteration 112, loss = 0.27000562\n",
            "Iteration 113, loss = 0.27008281\n",
            "Iteration 114, loss = 0.26905902\n",
            "Iteration 115, loss = 0.26999992\n",
            "Iteration 116, loss = 0.26839026\n",
            "Iteration 117, loss = 0.26789062\n",
            "Iteration 118, loss = 0.26669878\n",
            "Iteration 119, loss = 0.26555700\n",
            "Iteration 120, loss = 0.26769975\n",
            "Iteration 121, loss = 0.26423378\n",
            "Iteration 122, loss = 0.26551679\n",
            "Iteration 123, loss = 0.26258521\n",
            "Iteration 124, loss = 0.26430455\n",
            "Iteration 125, loss = 0.26209774\n",
            "Iteration 126, loss = 0.26106245\n",
            "Iteration 127, loss = 0.26231032\n",
            "Iteration 128, loss = 0.25874591\n",
            "Iteration 129, loss = 0.26092586\n",
            "Iteration 130, loss = 0.25801594\n",
            "Iteration 131, loss = 0.25678179\n",
            "Iteration 132, loss = 0.25706655\n",
            "Iteration 133, loss = 0.25643775\n",
            "Iteration 134, loss = 0.25557884\n",
            "Iteration 135, loss = 0.25466683\n",
            "Iteration 136, loss = 0.25429979\n",
            "Iteration 137, loss = 0.25173891\n",
            "Iteration 138, loss = 0.25205504\n",
            "Iteration 139, loss = 0.25166894\n",
            "Iteration 140, loss = 0.24821176\n",
            "Iteration 141, loss = 0.25125015\n",
            "Iteration 142, loss = 0.25072128\n",
            "Iteration 143, loss = 0.24990687\n",
            "Iteration 144, loss = 0.24583728\n",
            "Iteration 145, loss = 0.24575702\n",
            "Iteration 146, loss = 0.24542438\n",
            "Iteration 147, loss = 0.24521239\n",
            "Iteration 148, loss = 0.24270510\n",
            "Iteration 149, loss = 0.24268424\n",
            "Iteration 150, loss = 0.24217106\n",
            "Iteration 151, loss = 0.24219697\n",
            "Iteration 152, loss = 0.24250508\n",
            "Iteration 153, loss = 0.24017773\n",
            "Iteration 154, loss = 0.23957154\n",
            "Iteration 155, loss = 0.24032251\n",
            "Iteration 156, loss = 0.23715971\n",
            "Iteration 157, loss = 0.23528527\n",
            "Iteration 158, loss = 0.23533952\n",
            "Iteration 159, loss = 0.23622081\n",
            "Iteration 160, loss = 0.23250176\n",
            "Iteration 161, loss = 0.23311287\n",
            "Iteration 162, loss = 0.23223150\n",
            "Iteration 163, loss = 0.23418769\n",
            "Iteration 164, loss = 0.22991182\n",
            "Iteration 165, loss = 0.22918242\n",
            "Iteration 166, loss = 0.22805132\n",
            "Iteration 167, loss = 0.22957501\n",
            "Iteration 168, loss = 0.22762458\n",
            "Iteration 169, loss = 0.22673076\n",
            "Iteration 170, loss = 0.22603660\n",
            "Iteration 171, loss = 0.22382504\n",
            "Iteration 172, loss = 0.22381864\n",
            "Iteration 173, loss = 0.22451617\n",
            "Iteration 174, loss = 0.22114676\n",
            "Iteration 175, loss = 0.22133301\n",
            "Iteration 176, loss = 0.21943323\n",
            "Iteration 177, loss = 0.21864368\n",
            "Iteration 178, loss = 0.21723126\n",
            "Iteration 179, loss = 0.21944736\n",
            "Iteration 180, loss = 0.21720067\n",
            "Iteration 181, loss = 0.21509383\n",
            "Iteration 182, loss = 0.21347157\n",
            "Iteration 183, loss = 0.21346038\n",
            "Iteration 184, loss = 0.21280861\n",
            "Iteration 185, loss = 0.21049085\n",
            "Iteration 186, loss = 0.21183504\n",
            "Iteration 187, loss = 0.20842140\n",
            "Iteration 188, loss = 0.20806374\n",
            "Iteration 189, loss = 0.20746513\n",
            "Iteration 190, loss = 0.20899417\n",
            "Iteration 191, loss = 0.20662695\n",
            "Iteration 192, loss = 0.20739117\n",
            "Iteration 193, loss = 0.20425139\n",
            "Iteration 194, loss = 0.20365669\n",
            "Iteration 195, loss = 0.20420306\n",
            "Iteration 196, loss = 0.20259816\n",
            "Iteration 197, loss = 0.20052169\n",
            "Iteration 198, loss = 0.20033202\n",
            "Iteration 199, loss = 0.19852837\n",
            "Iteration 200, loss = 0.19878519\n",
            "Iteration 201, loss = 0.19707408\n",
            "Iteration 202, loss = 0.19652880\n",
            "Iteration 203, loss = 0.19693192\n",
            "Iteration 204, loss = 0.19585657\n",
            "Iteration 205, loss = 0.19296675\n",
            "Iteration 206, loss = 0.19208726\n",
            "Iteration 207, loss = 0.19237511\n",
            "Iteration 208, loss = 0.19315518\n",
            "Iteration 209, loss = 0.19152819\n",
            "Iteration 210, loss = 0.18903073\n",
            "Iteration 211, loss = 0.18954834\n",
            "Iteration 212, loss = 0.18875744\n",
            "Iteration 213, loss = 0.18819134\n",
            "Iteration 214, loss = 0.18672378\n",
            "Iteration 215, loss = 0.18534630\n",
            "Iteration 216, loss = 0.18566752\n",
            "Iteration 217, loss = 0.18582891\n",
            "Iteration 218, loss = 0.18509903\n",
            "Iteration 219, loss = 0.18422501\n",
            "Iteration 220, loss = 0.18296225\n",
            "Iteration 221, loss = 0.18335036\n",
            "Iteration 222, loss = 0.18192476\n",
            "Iteration 223, loss = 0.17920969\n",
            "Iteration 224, loss = 0.17781067\n",
            "Iteration 225, loss = 0.17866876\n",
            "Iteration 226, loss = 0.17816371\n",
            "Iteration 227, loss = 0.17647001\n",
            "Iteration 228, loss = 0.17613057\n",
            "Iteration 229, loss = 0.17612261\n",
            "Iteration 230, loss = 0.17322733\n",
            "Iteration 231, loss = 0.17444507\n",
            "Iteration 232, loss = 0.17240322\n",
            "Iteration 233, loss = 0.17287443\n",
            "Iteration 234, loss = 0.17185516\n",
            "Iteration 235, loss = 0.17184808\n",
            "Iteration 236, loss = 0.17233829\n",
            "Iteration 237, loss = 0.16922538\n",
            "Iteration 238, loss = 0.16901928\n",
            "Iteration 239, loss = 0.16782417\n",
            "Iteration 240, loss = 0.16790913\n",
            "Iteration 241, loss = 0.16591726\n",
            "Iteration 242, loss = 0.16673275\n",
            "Iteration 243, loss = 0.16594125\n",
            "Iteration 244, loss = 0.16647978\n",
            "Iteration 245, loss = 0.16433836\n",
            "Iteration 246, loss = 0.16415572\n",
            "Iteration 247, loss = 0.16362963\n",
            "Iteration 248, loss = 0.16260656\n",
            "Iteration 249, loss = 0.16135701\n",
            "Iteration 250, loss = 0.16165301\n",
            "Iteration 251, loss = 0.16042423\n",
            "Iteration 252, loss = 0.16090421\n",
            "Iteration 253, loss = 0.16028844\n",
            "Iteration 254, loss = 0.15832689\n",
            "Iteration 255, loss = 0.15716593\n",
            "Iteration 256, loss = 0.15858584\n",
            "Iteration 257, loss = 0.15964541\n",
            "Iteration 258, loss = 0.15727606\n",
            "Iteration 259, loss = 0.15863876\n",
            "Iteration 260, loss = 0.15563960\n",
            "Iteration 261, loss = 0.15502073\n",
            "Iteration 262, loss = 0.15426470\n",
            "Iteration 263, loss = 0.15274501\n",
            "Iteration 264, loss = 0.15337190\n",
            "Iteration 265, loss = 0.15219140\n",
            "Iteration 266, loss = 0.15251328\n",
            "Iteration 267, loss = 0.15209745\n",
            "Iteration 268, loss = 0.15315284\n",
            "Iteration 269, loss = 0.14923070\n",
            "Iteration 270, loss = 0.15000325\n",
            "Iteration 271, loss = 0.14940918\n",
            "Iteration 272, loss = 0.15056943\n",
            "Iteration 273, loss = 0.14845516\n",
            "Iteration 274, loss = 0.14738929\n",
            "Iteration 275, loss = 0.14885465\n",
            "Iteration 276, loss = 0.14735890\n",
            "Iteration 277, loss = 0.14682461\n",
            "Iteration 278, loss = 0.14729274\n",
            "Iteration 279, loss = 0.14732275\n",
            "Iteration 280, loss = 0.14680465\n",
            "Iteration 281, loss = 0.14393186\n",
            "Iteration 282, loss = 0.14507446\n",
            "Iteration 283, loss = 0.14253790\n",
            "Iteration 284, loss = 0.14249957\n",
            "Iteration 285, loss = 0.14236482\n",
            "Iteration 286, loss = 0.14415649\n",
            "Iteration 287, loss = 0.14201785\n",
            "Iteration 288, loss = 0.14163264\n",
            "Iteration 289, loss = 0.14444212\n",
            "Iteration 290, loss = 0.14035090\n",
            "Iteration 291, loss = 0.13967527\n",
            "Iteration 292, loss = 0.14068387\n",
            "Iteration 293, loss = 0.14166129\n",
            "Iteration 294, loss = 0.13894082\n",
            "Iteration 295, loss = 0.13795154\n",
            "Iteration 296, loss = 0.13747967\n",
            "Iteration 297, loss = 0.13712953\n",
            "Iteration 298, loss = 0.13811360\n",
            "Iteration 299, loss = 0.13725304\n",
            "Iteration 300, loss = 0.13672246\n",
            "Iteration 301, loss = 0.13599586\n",
            "Iteration 302, loss = 0.13585360\n",
            "Iteration 303, loss = 0.13627756\n",
            "Iteration 304, loss = 0.13673659\n",
            "Iteration 305, loss = 0.13494006\n",
            "Iteration 306, loss = 0.13598235\n",
            "Iteration 307, loss = 0.13500785\n",
            "Iteration 308, loss = 0.13494496\n",
            "Iteration 309, loss = 0.13339920\n",
            "Iteration 310, loss = 0.13322764\n",
            "Iteration 311, loss = 0.13231796\n",
            "Iteration 312, loss = 0.13207144\n",
            "Iteration 313, loss = 0.13119835\n",
            "Iteration 314, loss = 0.13057826\n",
            "Iteration 315, loss = 0.13127076\n",
            "Iteration 316, loss = 0.13343406\n",
            "Iteration 317, loss = 0.13038009\n",
            "Iteration 318, loss = 0.13010862\n",
            "Iteration 319, loss = 0.12944408\n",
            "Iteration 320, loss = 0.12871628\n",
            "Iteration 321, loss = 0.13055949\n",
            "Iteration 322, loss = 0.12933281\n",
            "Iteration 323, loss = 0.12902831\n",
            "Iteration 324, loss = 0.12890781\n",
            "Iteration 325, loss = 0.12916638\n",
            "Iteration 326, loss = 0.12693557\n",
            "Iteration 327, loss = 0.12708735\n",
            "Iteration 328, loss = 0.13016717\n",
            "Iteration 329, loss = 0.12557121\n",
            "Iteration 330, loss = 0.12687732\n",
            "Iteration 331, loss = 0.12681710\n",
            "Iteration 332, loss = 0.12785502\n",
            "Iteration 333, loss = 0.12474350\n",
            "Iteration 334, loss = 0.12536212\n",
            "Iteration 335, loss = 0.12679502\n",
            "Iteration 336, loss = 0.12433205\n",
            "Iteration 337, loss = 0.12359911\n",
            "Iteration 338, loss = 0.12429053\n",
            "Iteration 339, loss = 0.12358348\n",
            "Iteration 340, loss = 0.12278609\n",
            "Iteration 341, loss = 0.12297285\n",
            "Iteration 342, loss = 0.12329889\n",
            "Iteration 343, loss = 0.12284793\n",
            "Iteration 344, loss = 0.12260738\n",
            "Iteration 345, loss = 0.12377938\n",
            "Iteration 346, loss = 0.12311681\n",
            "Iteration 347, loss = 0.12280867\n",
            "Iteration 348, loss = 0.12224841\n",
            "Iteration 349, loss = 0.12205136\n",
            "Iteration 350, loss = 0.12165431\n",
            "Iteration 351, loss = 0.12228406\n",
            "Iteration 352, loss = 0.12164693\n",
            "Iteration 353, loss = 0.12116791\n",
            "Iteration 354, loss = 0.11968239\n",
            "Iteration 355, loss = 0.12014374\n",
            "Iteration 356, loss = 0.11993981\n",
            "Iteration 357, loss = 0.11943770\n",
            "Iteration 358, loss = 0.11843067\n",
            "Iteration 359, loss = 0.12029079\n",
            "Iteration 360, loss = 0.11882718\n",
            "Iteration 361, loss = 0.11906286\n",
            "Iteration 362, loss = 0.11930788\n",
            "Iteration 363, loss = 0.11732255\n",
            "Iteration 364, loss = 0.11760454\n",
            "Iteration 365, loss = 0.11746825\n",
            "Iteration 366, loss = 0.11928202\n",
            "Iteration 367, loss = 0.11669877\n",
            "Iteration 368, loss = 0.12000602\n",
            "Iteration 369, loss = 0.11743555\n",
            "Iteration 370, loss = 0.11614609\n",
            "Iteration 371, loss = 0.11588351\n",
            "Iteration 372, loss = 0.11581459\n",
            "Iteration 373, loss = 0.11593946\n",
            "Iteration 374, loss = 0.11578886\n",
            "Iteration 375, loss = 0.11702116\n",
            "Iteration 376, loss = 0.11594756\n",
            "Iteration 377, loss = 0.11606458\n",
            "Iteration 378, loss = 0.11596481\n",
            "Iteration 379, loss = 0.11388975\n",
            "Iteration 380, loss = 0.11434572\n",
            "Iteration 381, loss = 0.11507673\n",
            "Iteration 382, loss = 0.11443334\n",
            "Iteration 383, loss = 0.11724938\n",
            "Iteration 384, loss = 0.11477783\n",
            "Iteration 385, loss = 0.11293059\n",
            "Iteration 386, loss = 0.11401792\n",
            "Iteration 387, loss = 0.11406665\n",
            "Iteration 388, loss = 0.11343294\n",
            "Iteration 389, loss = 0.11328098\n",
            "Iteration 390, loss = 0.11249678\n",
            "Iteration 391, loss = 0.11321110\n",
            "Iteration 392, loss = 0.11418435\n",
            "Iteration 393, loss = 0.11061747\n",
            "Iteration 394, loss = 0.11282931\n",
            "Iteration 395, loss = 0.11157619\n",
            "Iteration 396, loss = 0.11213556\n",
            "Iteration 397, loss = 0.11077192\n",
            "Iteration 398, loss = 0.11098197\n",
            "Iteration 399, loss = 0.10994251\n",
            "Iteration 400, loss = 0.11079098\n",
            "Iteration 401, loss = 0.11089218\n",
            "Iteration 402, loss = 0.11293760\n",
            "Iteration 403, loss = 0.10914320\n",
            "Iteration 404, loss = 0.11140629\n",
            "Iteration 405, loss = 0.11245413\n",
            "Iteration 406, loss = 0.11090561\n",
            "Iteration 407, loss = 0.11089741\n",
            "Iteration 408, loss = 0.10983110\n",
            "Iteration 409, loss = 0.11194037\n",
            "Iteration 410, loss = 0.11082871\n",
            "Iteration 411, loss = 0.11045048\n",
            "Iteration 412, loss = 0.10801592\n",
            "Iteration 413, loss = 0.10926125\n",
            "Iteration 414, loss = 0.10940450\n",
            "Iteration 415, loss = 0.10787933\n",
            "Iteration 416, loss = 0.11350257\n",
            "Iteration 417, loss = 0.10891116\n",
            "Iteration 418, loss = 0.10971310\n",
            "Iteration 419, loss = 0.11012596\n",
            "Iteration 420, loss = 0.10584126\n",
            "Iteration 421, loss = 0.10934187\n",
            "Iteration 422, loss = 0.10886819\n",
            "Iteration 423, loss = 0.10627708\n",
            "Iteration 424, loss = 0.10735660\n",
            "Iteration 425, loss = 0.10754358\n",
            "Iteration 426, loss = 0.10757193\n",
            "Iteration 427, loss = 0.10886795\n",
            "Iteration 428, loss = 0.10712495\n",
            "Iteration 429, loss = 0.10808085\n",
            "Iteration 430, loss = 0.10592941\n",
            "Iteration 431, loss = 0.10550660\n",
            "Iteration 432, loss = 0.10555670\n",
            "Iteration 433, loss = 0.10586662\n",
            "Iteration 434, loss = 0.10445231\n",
            "Iteration 435, loss = 0.10590744\n",
            "Iteration 436, loss = 0.10434532\n",
            "Iteration 437, loss = 0.10516802\n",
            "Iteration 438, loss = 0.10583282\n",
            "Iteration 439, loss = 0.10403313\n",
            "Iteration 440, loss = 0.10412158\n",
            "Iteration 441, loss = 0.10589873\n",
            "Iteration 442, loss = 0.10407373\n",
            "Iteration 443, loss = 0.10420264\n",
            "Iteration 444, loss = 0.10261031\n",
            "Iteration 445, loss = 0.10268973\n",
            "Iteration 446, loss = 0.10360988\n",
            "Iteration 447, loss = 0.10512191\n",
            "Iteration 448, loss = 0.10353179\n",
            "Iteration 449, loss = 0.10437276\n",
            "Iteration 450, loss = 0.10235972\n",
            "Iteration 451, loss = 0.10325449\n",
            "Iteration 452, loss = 0.10350759\n",
            "Iteration 453, loss = 0.10419059\n",
            "Iteration 454, loss = 0.10226976\n",
            "Iteration 455, loss = 0.10409326\n",
            "Iteration 456, loss = 0.10340755\n",
            "Iteration 457, loss = 0.10145478\n",
            "Iteration 458, loss = 0.10282872\n",
            "Iteration 459, loss = 0.10243007\n",
            "Iteration 460, loss = 0.10109533\n",
            "Iteration 461, loss = 0.10252833\n",
            "Iteration 462, loss = 0.10029496\n",
            "Iteration 463, loss = 0.10105631\n",
            "Iteration 464, loss = 0.10186728\n",
            "Iteration 465, loss = 0.10094835\n",
            "Iteration 466, loss = 0.10133067\n",
            "Iteration 467, loss = 0.10124607\n",
            "Iteration 468, loss = 0.09935196\n",
            "Iteration 469, loss = 0.10260360\n",
            "Iteration 470, loss = 0.10211092\n",
            "Iteration 471, loss = 0.10012191\n",
            "Iteration 472, loss = 0.10000667\n",
            "Iteration 473, loss = 0.09948491\n",
            "Iteration 474, loss = 0.10116166\n",
            "Iteration 475, loss = 0.10059226\n",
            "Iteration 476, loss = 0.10231599\n",
            "Iteration 477, loss = 0.10011991\n",
            "Iteration 478, loss = 0.09965155\n",
            "Iteration 479, loss = 0.09926620\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.782064\n",
            "Test set score: -2.891244\n",
            "Mean squared error 4.542997\n",
            "Variance score -2.891244\n",
            "Mean absolute error 1.909255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOfR5rgzrBx",
        "colab_type": "code",
        "outputId": "99e34cd3-7607-4655-cfde-335963703312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.894943\n",
            "Test set score: 0.879055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynbW_GFAnksU",
        "colab_type": "code",
        "outputId": "8542efc8-e427-4071-895c-0004fd822fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2621
        }
      },
      "source": [
        "model= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='adam', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.41443947\n",
            "Iteration 2, loss = 0.38136363\n",
            "Iteration 3, loss = 0.36144322\n",
            "Iteration 4, loss = 0.36341959\n",
            "Iteration 5, loss = 0.36290796\n",
            "Iteration 6, loss = 0.35709987\n",
            "Iteration 7, loss = 0.34967758\n",
            "Iteration 8, loss = 0.34618839\n",
            "Iteration 9, loss = 0.34188624\n",
            "Iteration 10, loss = 0.33254513\n",
            "Iteration 11, loss = 0.33528213\n",
            "Iteration 12, loss = 0.32982294\n",
            "Iteration 13, loss = 0.32284667\n",
            "Iteration 14, loss = 0.32096061\n",
            "Iteration 15, loss = 0.31939042\n",
            "Iteration 16, loss = 0.31522251\n",
            "Iteration 17, loss = 0.31556593\n",
            "Iteration 18, loss = 0.31776981\n",
            "Iteration 19, loss = 0.31641474\n",
            "Iteration 20, loss = 0.30783263\n",
            "Iteration 21, loss = 0.30292969\n",
            "Iteration 22, loss = 0.30011261\n",
            "Iteration 23, loss = 0.30654710\n",
            "Iteration 24, loss = 0.29095640\n",
            "Iteration 25, loss = 0.29066261\n",
            "Iteration 26, loss = 0.28721580\n",
            "Iteration 27, loss = 0.28210434\n",
            "Iteration 28, loss = 0.27805439\n",
            "Iteration 29, loss = 0.26882481\n",
            "Iteration 30, loss = 0.26703972\n",
            "Iteration 31, loss = 0.26322170\n",
            "Iteration 32, loss = 0.25638033\n",
            "Iteration 33, loss = 0.26053717\n",
            "Iteration 34, loss = 0.25061131\n",
            "Iteration 35, loss = 0.24226023\n",
            "Iteration 36, loss = 0.24166227\n",
            "Iteration 37, loss = 0.23447571\n",
            "Iteration 38, loss = 0.23567611\n",
            "Iteration 39, loss = 0.23443704\n",
            "Iteration 40, loss = 0.23380433\n",
            "Iteration 41, loss = 0.22831078\n",
            "Iteration 42, loss = 0.23181612\n",
            "Iteration 43, loss = 0.22207029\n",
            "Iteration 44, loss = 0.22564121\n",
            "Iteration 45, loss = 0.22310738\n",
            "Iteration 46, loss = 0.21891744\n",
            "Iteration 47, loss = 0.21444884\n",
            "Iteration 48, loss = 0.20804685\n",
            "Iteration 49, loss = 0.20435674\n",
            "Iteration 50, loss = 0.20131741\n",
            "Iteration 51, loss = 0.20986262\n",
            "Iteration 52, loss = 0.20686450\n",
            "Iteration 53, loss = 0.21064147\n",
            "Iteration 54, loss = 0.21299191\n",
            "Iteration 55, loss = 0.20131303\n",
            "Iteration 56, loss = 0.19768716\n",
            "Iteration 57, loss = 0.20648276\n",
            "Iteration 58, loss = 0.20948721\n",
            "Iteration 59, loss = 0.20538140\n",
            "Iteration 60, loss = 0.20544316\n",
            "Iteration 61, loss = 0.19171674\n",
            "Iteration 62, loss = 0.19103360\n",
            "Iteration 63, loss = 0.19345054\n",
            "Iteration 64, loss = 0.18429688\n",
            "Iteration 65, loss = 0.18185292\n",
            "Iteration 66, loss = 0.18095794\n",
            "Iteration 67, loss = 0.20462207\n",
            "Iteration 68, loss = 0.19255516\n",
            "Iteration 69, loss = 0.17634809\n",
            "Iteration 70, loss = 0.17772734\n",
            "Iteration 71, loss = 0.17577261\n",
            "Iteration 72, loss = 0.17493105\n",
            "Iteration 73, loss = 0.17284196\n",
            "Iteration 74, loss = 0.16726515\n",
            "Iteration 75, loss = 0.17128006\n",
            "Iteration 76, loss = 0.16329146\n",
            "Iteration 77, loss = 0.16526101\n",
            "Iteration 78, loss = 0.16872686\n",
            "Iteration 79, loss = 0.17197896\n",
            "Iteration 80, loss = 0.17302865\n",
            "Iteration 81, loss = 0.17000936\n",
            "Iteration 82, loss = 0.16924543\n",
            "Iteration 83, loss = 0.16803051\n",
            "Iteration 84, loss = 0.18172146\n",
            "Iteration 85, loss = 0.16780096\n",
            "Iteration 86, loss = 0.16792333\n",
            "Iteration 87, loss = 0.16064261\n",
            "Iteration 88, loss = 0.16375990\n",
            "Iteration 89, loss = 0.16234702\n",
            "Iteration 90, loss = 0.15829162\n",
            "Iteration 91, loss = 0.15780815\n",
            "Iteration 92, loss = 0.16062143\n",
            "Iteration 93, loss = 0.15919880\n",
            "Iteration 94, loss = 0.17764816\n",
            "Iteration 95, loss = 0.15649558\n",
            "Iteration 96, loss = 0.15602089\n",
            "Iteration 97, loss = 0.15598386\n",
            "Iteration 98, loss = 0.15552447\n",
            "Iteration 99, loss = 0.15345909\n",
            "Iteration 100, loss = 0.15309571\n",
            "Iteration 101, loss = 0.15798745\n",
            "Iteration 102, loss = 0.15958284\n",
            "Iteration 103, loss = 0.15637206\n",
            "Iteration 104, loss = 0.16100356\n",
            "Iteration 105, loss = 0.15815832\n",
            "Iteration 106, loss = 0.14982862\n",
            "Iteration 107, loss = 0.14777986\n",
            "Iteration 108, loss = 0.14908856\n",
            "Iteration 109, loss = 0.14591868\n",
            "Iteration 110, loss = 0.15255867\n",
            "Iteration 111, loss = 0.16029949\n",
            "Iteration 112, loss = 0.15368812\n",
            "Iteration 113, loss = 0.15223647\n",
            "Iteration 114, loss = 0.15757553\n",
            "Iteration 115, loss = 0.15163109\n",
            "Iteration 116, loss = 0.16264652\n",
            "Iteration 117, loss = 0.15787298\n",
            "Iteration 118, loss = 0.14606202\n",
            "Iteration 119, loss = 0.14398107\n",
            "Iteration 120, loss = 0.14274580\n",
            "Iteration 121, loss = 0.14875348\n",
            "Iteration 122, loss = 0.15086670\n",
            "Iteration 123, loss = 0.14615227\n",
            "Iteration 124, loss = 0.14303824\n",
            "Iteration 125, loss = 0.14563028\n",
            "Iteration 126, loss = 0.14734775\n",
            "Iteration 127, loss = 0.14407966\n",
            "Iteration 128, loss = 0.14251156\n",
            "Iteration 129, loss = 0.14080018\n",
            "Iteration 130, loss = 0.13611077\n",
            "Iteration 131, loss = 0.13974838\n",
            "Iteration 132, loss = 0.14692949\n",
            "Iteration 133, loss = 0.14343654\n",
            "Iteration 134, loss = 0.13815012\n",
            "Iteration 135, loss = 0.13888104\n",
            "Iteration 136, loss = 0.13625727\n",
            "Iteration 137, loss = 0.13931490\n",
            "Iteration 138, loss = 0.13623490\n",
            "Iteration 139, loss = 0.14348279\n",
            "Iteration 140, loss = 0.13685666\n",
            "Iteration 141, loss = 0.13264945\n",
            "Iteration 142, loss = 0.14216461\n",
            "Iteration 143, loss = 0.13538257\n",
            "Iteration 144, loss = 0.13941417\n",
            "Iteration 145, loss = 0.13753426\n",
            "Iteration 146, loss = 0.14778748\n",
            "Iteration 147, loss = 0.14268787\n",
            "Iteration 148, loss = 0.13952334\n",
            "Iteration 149, loss = 0.15296986\n",
            "Iteration 150, loss = 0.14976908\n",
            "Iteration 151, loss = 0.14224767\n",
            "Iteration 152, loss = 0.14028760\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.784525\n",
            "Test set score: 0.516757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOd4zC2LI4CT",
        "colab_type": "code",
        "outputId": "9fe0a0bf-cfb5-45a0-cad3-ecbf07628bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.002, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='adaptive', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.886606\n",
            "Test set score: 0.880719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGJWC3SNV0uE",
        "colab_type": "code",
        "outputId": "1fdb5cc0-798b-47cb-b3a3-19ccaeca0eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.002, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='adaptive', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1500, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.883342\n",
            "Test set score: 0.844656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u2NxeOxJ7Fb",
        "colab_type": "code",
        "outputId": "7c3c64e9-1be3-464e-dc6f-751a7176836e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.0034, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='adaptive', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.891822\n",
            "Test set score: 0.880960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qdPExyCay7c",
        "colab_type": "code",
        "outputId": "d6e19b49-ddbf-4414-82e4-3d5ed7eef4cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.002, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.887747\n",
            "Test set score: 0.843761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXQ72ACE-min",
        "colab_type": "code",
        "outputId": "9244bba0-8c62-4417-ac9b-5414e9741c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2835
        }
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='adam', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
        "\n",
        "pd.DataFrame(mlp.loss_curve_).plot()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.43699419\n",
            "Iteration 2, loss = 0.37387916\n",
            "Iteration 3, loss = 0.36254365\n",
            "Iteration 4, loss = 0.36142931\n",
            "Iteration 5, loss = 0.35953018\n",
            "Iteration 6, loss = 0.34887587\n",
            "Iteration 7, loss = 0.34318587\n",
            "Iteration 8, loss = 0.34449825\n",
            "Iteration 9, loss = 0.33496432\n",
            "Iteration 10, loss = 0.33437573\n",
            "Iteration 11, loss = 0.32935752\n",
            "Iteration 12, loss = 0.32785045\n",
            "Iteration 13, loss = 0.32763110\n",
            "Iteration 14, loss = 0.32390422\n",
            "Iteration 15, loss = 0.32752154\n",
            "Iteration 16, loss = 0.32213494\n",
            "Iteration 17, loss = 0.32972155\n",
            "Iteration 18, loss = 0.33476986\n",
            "Iteration 19, loss = 0.31915784\n",
            "Iteration 20, loss = 0.31510514\n",
            "Iteration 21, loss = 0.30767194\n",
            "Iteration 22, loss = 0.30685550\n",
            "Iteration 23, loss = 0.31083531\n",
            "Iteration 24, loss = 0.30870782\n",
            "Iteration 25, loss = 0.30381318\n",
            "Iteration 26, loss = 0.29693674\n",
            "Iteration 27, loss = 0.30597159\n",
            "Iteration 28, loss = 0.30776298\n",
            "Iteration 29, loss = 0.29177153\n",
            "Iteration 30, loss = 0.28208367\n",
            "Iteration 31, loss = 0.28799750\n",
            "Iteration 32, loss = 0.27787630\n",
            "Iteration 33, loss = 0.27166163\n",
            "Iteration 34, loss = 0.27622832\n",
            "Iteration 35, loss = 0.26806884\n",
            "Iteration 36, loss = 0.26770398\n",
            "Iteration 37, loss = 0.26567330\n",
            "Iteration 38, loss = 0.27353212\n",
            "Iteration 39, loss = 0.25187040\n",
            "Iteration 40, loss = 0.26963408\n",
            "Iteration 41, loss = 0.27903949\n",
            "Iteration 42, loss = 0.25682964\n",
            "Iteration 43, loss = 0.25265499\n",
            "Iteration 44, loss = 0.24840720\n",
            "Iteration 45, loss = 0.23949651\n",
            "Iteration 46, loss = 0.23168371\n",
            "Iteration 47, loss = 0.23468027\n",
            "Iteration 48, loss = 0.23826710\n",
            "Iteration 49, loss = 0.23076615\n",
            "Iteration 50, loss = 0.23040300\n",
            "Iteration 51, loss = 0.22387864\n",
            "Iteration 52, loss = 0.21858518\n",
            "Iteration 53, loss = 0.21346882\n",
            "Iteration 54, loss = 0.21012580\n",
            "Iteration 55, loss = 0.21730941\n",
            "Iteration 56, loss = 0.21345855\n",
            "Iteration 57, loss = 0.22261389\n",
            "Iteration 58, loss = 0.21440083\n",
            "Iteration 59, loss = 0.20293950\n",
            "Iteration 60, loss = 0.19886461\n",
            "Iteration 61, loss = 0.19785523\n",
            "Iteration 62, loss = 0.20212824\n",
            "Iteration 63, loss = 0.19985098\n",
            "Iteration 64, loss = 0.19916096\n",
            "Iteration 65, loss = 0.20164075\n",
            "Iteration 66, loss = 0.21096730\n",
            "Iteration 67, loss = 0.19063183\n",
            "Iteration 68, loss = 0.19111717\n",
            "Iteration 69, loss = 0.19171467\n",
            "Iteration 70, loss = 0.19481860\n",
            "Iteration 71, loss = 0.18863177\n",
            "Iteration 72, loss = 0.18322003\n",
            "Iteration 73, loss = 0.19468278\n",
            "Iteration 74, loss = 0.18009011\n",
            "Iteration 75, loss = 0.18632322\n",
            "Iteration 76, loss = 0.19084406\n",
            "Iteration 77, loss = 0.17553020\n",
            "Iteration 78, loss = 0.17930231\n",
            "Iteration 79, loss = 0.18185943\n",
            "Iteration 80, loss = 0.17522899\n",
            "Iteration 81, loss = 0.17331163\n",
            "Iteration 82, loss = 0.17877340\n",
            "Iteration 83, loss = 0.19451731\n",
            "Iteration 84, loss = 0.19658615\n",
            "Iteration 85, loss = 0.18630854\n",
            "Iteration 86, loss = 0.16865378\n",
            "Iteration 87, loss = 0.16767803\n",
            "Iteration 88, loss = 0.17349516\n",
            "Iteration 89, loss = 0.15977596\n",
            "Iteration 90, loss = 0.17024533\n",
            "Iteration 91, loss = 0.16758919\n",
            "Iteration 92, loss = 0.17927800\n",
            "Iteration 93, loss = 0.16794164\n",
            "Iteration 94, loss = 0.16315787\n",
            "Iteration 95, loss = 0.16737640\n",
            "Iteration 96, loss = 0.16577393\n",
            "Iteration 97, loss = 0.17986915\n",
            "Iteration 98, loss = 0.16453039\n",
            "Iteration 99, loss = 0.15479486\n",
            "Iteration 100, loss = 0.16211986\n",
            "Iteration 101, loss = 0.16453512\n",
            "Iteration 102, loss = 0.16256606\n",
            "Iteration 103, loss = 0.15191621\n",
            "Iteration 104, loss = 0.16247141\n",
            "Iteration 105, loss = 0.15522819\n",
            "Iteration 106, loss = 0.15725497\n",
            "Iteration 107, loss = 0.15085257\n",
            "Iteration 108, loss = 0.15121375\n",
            "Iteration 109, loss = 0.15441497\n",
            "Iteration 110, loss = 0.15336491\n",
            "Iteration 111, loss = 0.14880075\n",
            "Iteration 112, loss = 0.15577582\n",
            "Iteration 113, loss = 0.15818585\n",
            "Iteration 114, loss = 0.15340104\n",
            "Iteration 115, loss = 0.15532088\n",
            "Iteration 116, loss = 0.15662182\n",
            "Iteration 117, loss = 0.15711781\n",
            "Iteration 118, loss = 0.15143125\n",
            "Iteration 119, loss = 0.15379900\n",
            "Iteration 120, loss = 0.14471259\n",
            "Iteration 121, loss = 0.14845298\n",
            "Iteration 122, loss = 0.20744956\n",
            "Iteration 123, loss = 0.18103598\n",
            "Iteration 124, loss = 0.14833842\n",
            "Iteration 125, loss = 0.14636650\n",
            "Iteration 126, loss = 0.14303325\n",
            "Iteration 127, loss = 0.15105551\n",
            "Iteration 128, loss = 0.15653423\n",
            "Iteration 129, loss = 0.14456850\n",
            "Iteration 130, loss = 0.16257206\n",
            "Iteration 131, loss = 0.14509426\n",
            "Iteration 132, loss = 0.14379956\n",
            "Iteration 133, loss = 0.13961722\n",
            "Iteration 134, loss = 0.14438420\n",
            "Iteration 135, loss = 0.14475024\n",
            "Iteration 136, loss = 0.14101908\n",
            "Iteration 137, loss = 0.14314283\n",
            "Iteration 138, loss = 0.14111832\n",
            "Iteration 139, loss = 0.15183197\n",
            "Iteration 140, loss = 0.14272955\n",
            "Iteration 141, loss = 0.14382464\n",
            "Iteration 142, loss = 0.14075120\n",
            "Iteration 143, loss = 0.14518608\n",
            "Iteration 144, loss = 0.14361934\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.769245\n",
            "Test set score: 0.568731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5fe96bfb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFMCAYAAADx1nR5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzPByZowxxmSGlzNjjDEmM38Ao9w/jzLQrdcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRDbXN9jYSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='adam', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_STGaEojjrH",
        "colab_type": "code",
        "outputId": "1bbf6f22-8d71-41ea-93d5-abf7a8ffa341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3044
        }
      },
      "source": [
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.41752889\n",
            "Iteration 2, loss = 0.37615019\n",
            "Iteration 3, loss = 0.35900622\n",
            "Iteration 4, loss = 0.35735335\n",
            "Iteration 5, loss = 0.34865020\n",
            "Iteration 6, loss = 0.34727512\n",
            "Iteration 7, loss = 0.34274542\n",
            "Iteration 8, loss = 0.35043430\n",
            "Iteration 9, loss = 0.33874546\n",
            "Iteration 10, loss = 0.33759300\n",
            "Iteration 11, loss = 0.33102313\n",
            "Iteration 12, loss = 0.32378827\n",
            "Iteration 13, loss = 0.32471500\n",
            "Iteration 14, loss = 0.32336651\n",
            "Iteration 15, loss = 0.32036030\n",
            "Iteration 16, loss = 0.31595016\n",
            "Iteration 17, loss = 0.31345986\n",
            "Iteration 18, loss = 0.31099318\n",
            "Iteration 19, loss = 0.30696811\n",
            "Iteration 20, loss = 0.31601036\n",
            "Iteration 21, loss = 0.32337516\n",
            "Iteration 22, loss = 0.30504398\n",
            "Iteration 23, loss = 0.30118165\n",
            "Iteration 24, loss = 0.29901465\n",
            "Iteration 25, loss = 0.29377483\n",
            "Iteration 26, loss = 0.29577285\n",
            "Iteration 27, loss = 0.28571216\n",
            "Iteration 28, loss = 0.28754892\n",
            "Iteration 29, loss = 0.27567178\n",
            "Iteration 30, loss = 0.28164392\n",
            "Iteration 31, loss = 0.27369070\n",
            "Iteration 32, loss = 0.26950453\n",
            "Iteration 33, loss = 0.26596873\n",
            "Iteration 34, loss = 0.27128596\n",
            "Iteration 35, loss = 0.27194472\n",
            "Iteration 36, loss = 0.25951039\n",
            "Iteration 37, loss = 0.25948867\n",
            "Iteration 38, loss = 0.26159089\n",
            "Iteration 39, loss = 0.25141721\n",
            "Iteration 40, loss = 0.25211461\n",
            "Iteration 41, loss = 0.24850322\n",
            "Iteration 42, loss = 0.26098581\n",
            "Iteration 43, loss = 0.24268862\n",
            "Iteration 44, loss = 0.23829234\n",
            "Iteration 45, loss = 0.24394745\n",
            "Iteration 46, loss = 0.23781875\n",
            "Iteration 47, loss = 0.22622025\n",
            "Iteration 48, loss = 0.22592351\n",
            "Iteration 49, loss = 0.22785297\n",
            "Iteration 50, loss = 0.22234887\n",
            "Iteration 51, loss = 0.22663858\n",
            "Iteration 52, loss = 0.23330794\n",
            "Iteration 53, loss = 0.22050547\n",
            "Iteration 54, loss = 0.21066409\n",
            "Iteration 55, loss = 0.21757782\n",
            "Iteration 56, loss = 0.21426900\n",
            "Iteration 57, loss = 0.20863012\n",
            "Iteration 58, loss = 0.20248019\n",
            "Iteration 59, loss = 0.20896785\n",
            "Iteration 60, loss = 0.20156196\n",
            "Iteration 61, loss = 0.20347845\n",
            "Iteration 62, loss = 0.20967945\n",
            "Iteration 63, loss = 0.20017413\n",
            "Iteration 64, loss = 0.19114948\n",
            "Iteration 65, loss = 0.18871608\n",
            "Iteration 66, loss = 0.18683620\n",
            "Iteration 67, loss = 0.19333377\n",
            "Iteration 68, loss = 0.18386130\n",
            "Iteration 69, loss = 0.18017469\n",
            "Iteration 70, loss = 0.18750420\n",
            "Iteration 71, loss = 0.18401285\n",
            "Iteration 72, loss = 0.19066607\n",
            "Iteration 73, loss = 0.18368373\n",
            "Iteration 74, loss = 0.17878004\n",
            "Iteration 75, loss = 0.19153118\n",
            "Iteration 76, loss = 0.18041652\n",
            "Iteration 77, loss = 0.18821718\n",
            "Iteration 78, loss = 0.18066969\n",
            "Iteration 79, loss = 0.19141546\n",
            "Iteration 80, loss = 0.18872992\n",
            "Iteration 81, loss = 0.18289159\n",
            "Iteration 82, loss = 0.18067059\n",
            "Iteration 83, loss = 0.17841560\n",
            "Iteration 84, loss = 0.17101576\n",
            "Iteration 85, loss = 0.17412728\n",
            "Iteration 86, loss = 0.16678161\n",
            "Iteration 87, loss = 0.17211911\n",
            "Iteration 88, loss = 0.16298552\n",
            "Iteration 89, loss = 0.16166956\n",
            "Iteration 90, loss = 0.18697449\n",
            "Iteration 91, loss = 0.18842490\n",
            "Iteration 92, loss = 0.16200292\n",
            "Iteration 93, loss = 0.15816433\n",
            "Iteration 94, loss = 0.16789707\n",
            "Iteration 95, loss = 0.16636623\n",
            "Iteration 96, loss = 0.15931940\n",
            "Iteration 97, loss = 0.16528743\n",
            "Iteration 98, loss = 0.17696703\n",
            "Iteration 99, loss = 0.18635650\n",
            "Iteration 100, loss = 0.17178288\n",
            "Iteration 101, loss = 0.16355132\n",
            "Iteration 102, loss = 0.15696814\n",
            "Iteration 103, loss = 0.16165695\n",
            "Iteration 104, loss = 0.16159294\n",
            "Iteration 105, loss = 0.15577848\n",
            "Iteration 106, loss = 0.15864151\n",
            "Iteration 107, loss = 0.15508459\n",
            "Iteration 108, loss = 0.15288323\n",
            "Iteration 109, loss = 0.15261719\n",
            "Iteration 110, loss = 0.15378728\n",
            "Iteration 111, loss = 0.15259604\n",
            "Iteration 112, loss = 0.15149817\n",
            "Iteration 113, loss = 0.16018393\n",
            "Iteration 114, loss = 0.16950606\n",
            "Iteration 115, loss = 0.15721507\n",
            "Iteration 116, loss = 0.15859931\n",
            "Iteration 117, loss = 0.16068412\n",
            "Iteration 118, loss = 0.14946166\n",
            "Iteration 119, loss = 0.14989808\n",
            "Iteration 120, loss = 0.14913389\n",
            "Iteration 121, loss = 0.15589307\n",
            "Iteration 122, loss = 0.15746374\n",
            "Iteration 123, loss = 0.14955258\n",
            "Iteration 124, loss = 0.14776299\n",
            "Iteration 125, loss = 0.15907992\n",
            "Iteration 126, loss = 0.15282571\n",
            "Iteration 127, loss = 0.14483276\n",
            "Iteration 128, loss = 0.15071176\n",
            "Iteration 129, loss = 0.15804338\n",
            "Iteration 130, loss = 0.15407564\n",
            "Iteration 131, loss = 0.15688988\n",
            "Iteration 132, loss = 0.14412999\n",
            "Iteration 133, loss = 0.15267901\n",
            "Iteration 134, loss = 0.16510002\n",
            "Iteration 135, loss = 0.15945892\n",
            "Iteration 136, loss = 0.14779204\n",
            "Iteration 137, loss = 0.14990754\n",
            "Iteration 138, loss = 0.15394330\n",
            "Iteration 139, loss = 0.15137648\n",
            "Iteration 140, loss = 0.14943411\n",
            "Iteration 141, loss = 0.14150724\n",
            "Iteration 142, loss = 0.14321523\n",
            "Iteration 143, loss = 0.16335316\n",
            "Iteration 144, loss = 0.15179732\n",
            "Iteration 145, loss = 0.14684237\n",
            "Iteration 146, loss = 0.15491304\n",
            "Iteration 147, loss = 0.16344014\n",
            "Iteration 148, loss = 0.14762030\n",
            "Iteration 149, loss = 0.13871811\n",
            "Iteration 150, loss = 0.14232868\n",
            "Iteration 151, loss = 0.15749379\n",
            "Iteration 152, loss = 0.15110651\n",
            "Iteration 153, loss = 0.14211945\n",
            "Iteration 154, loss = 0.14141853\n",
            "Iteration 155, loss = 0.14162638\n",
            "Iteration 156, loss = 0.13910458\n",
            "Iteration 157, loss = 0.14770459\n",
            "Iteration 158, loss = 0.13655517\n",
            "Iteration 159, loss = 0.14135312\n",
            "Iteration 160, loss = 0.14040278\n",
            "Iteration 161, loss = 0.13838841\n",
            "Iteration 162, loss = 0.14334794\n",
            "Iteration 163, loss = 0.15375655\n",
            "Iteration 164, loss = 0.14541071\n",
            "Iteration 165, loss = 0.13970925\n",
            "Iteration 166, loss = 0.13949312\n",
            "Iteration 167, loss = 0.15102903\n",
            "Iteration 168, loss = 0.14430343\n",
            "Iteration 169, loss = 0.14386220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.780633\n",
            "Test set score: 0.584501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFt9-3czG4mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='lbfgs', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZLq3KrGAQ6V",
        "colab_type": "code",
        "outputId": "09f51e38-7da9-4dc8-b87e-a7b80c991746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "mlp.fit(X_scaled, y_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(X_scaled, y_scaled))\n",
        "print(\"Test set score: %f\" % mlp.score(X_testscaled, y_testscaled))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.884287\n",
            "Test set score: -42.692384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r3vUGKIk5n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(r'data.csv')\n",
        "\n",
        "X = df.drop('3', axis=1)\n",
        "w = X.drop('1', axis=1)\n",
        "o = w.drop('2', axis=1)\n",
        "\n",
        "p = df['3']\n",
        "\n",
        "o_train, o_test = o[:1600], o[1600:]\n",
        "p_train, p_test = p[:1600], p[1600:]\n",
        "\n",
        "from sklearn import preprocessing\n",
        "o_scaled = preprocessing.scale(o_train)\n",
        "p_scaled = preprocessing.scale(p_train)\n",
        "o_testscaled = preprocessing.scale(o_test)\n",
        "p_testscaled = preprocessing.scale(p_test)\n",
        "\n",
        "mlp= MLPRegressor(hidden_layer_sizes= (100,), \n",
        "                  activation='relu', \n",
        "                  solver='adam', \n",
        "                  alpha=0.0001, \n",
        "                  batch_size=128, \n",
        "                  learning_rate='constant', \n",
        "                  learning_rate_init=0.01,\n",
        "                  max_iter=1000, \n",
        "                  shuffle=True, \n",
        "                  random_state=None, \n",
        "                  verbose=True,\n",
        "                  warm_start=False,\n",
        "                  early_stopping=False, \n",
        "                  beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n",
        "\n",
        "mlp.fit(o_scaled, p_scaled)\n",
        "print(\"Training set score: %f\" % mlp.score(o_scaled, p_scaled))\n",
        "print(\"Test set score: %f\" % mlp.score(o_testscaled, p_testscaled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6FfR9GYbFxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(X_train, y_train)\n",
        "scaler.transform(X_train,y_train)\n",
        "scaler.transform(X_test, y_test)\n",
        "print(y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJqqhxLFcid9",
        "colab_type": "code",
        "outputId": "6ece4b2d-8965-401d-b13f-0a8a4255cfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "mlp.fit(X_train, y_train)\n",
        "print(mlp.score(X_train, y_train))\n",
        "print(mlp.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1515.31589428\n",
            "Iteration 2, loss = 286.64669246\n",
            "Iteration 3, loss = 33.28859666\n",
            "Iteration 4, loss = 19.41223372\n",
            "Iteration 5, loss = 6.44290105\n",
            "Iteration 6, loss = 1.17802482\n",
            "Iteration 7, loss = 0.26738360\n",
            "Iteration 8, loss = 0.07626135\n",
            "Iteration 9, loss = 0.01955588\n",
            "Iteration 10, loss = 0.00578584\n",
            "Iteration 11, loss = 0.00153681\n",
            "Iteration 12, loss = 0.00038244\n",
            "Iteration 13, loss = 0.00009999\n",
            "Iteration 14, loss = 0.00002623\n",
            "Iteration 15, loss = 0.00001379\n",
            "Iteration 16, loss = 0.00001004\n",
            "Iteration 17, loss = 0.00000850\n",
            "Iteration 18, loss = 0.00000773\n",
            "Iteration 19, loss = 0.00000759\n",
            "Iteration 20, loss = 0.00000749\n",
            "Iteration 21, loss = 0.00000742\n",
            "Iteration 22, loss = 0.00000736\n",
            "Iteration 23, loss = 0.00000731\n",
            "Iteration 24, loss = 0.00000726\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "-25415922111.427303\n",
            "-39677990099.65804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7r8z0WjKT4f",
        "colab_type": "code",
        "outputId": "1ef54803-d181-463f-ea04-1cccaddf8190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1082
        }
      },
      "source": [
        "mlp.fit(X_train, y_train)\n",
        "print(accuracy_score(X_train, y_train))\n",
        "print(accuracy_score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 9898.90992228\n",
            "Iteration 2, loss = 515.11148409\n",
            "Iteration 3, loss = 288.06607688\n",
            "Iteration 4, loss = 99.50872359\n",
            "Iteration 5, loss = 10.70825723\n",
            "Iteration 6, loss = 7.67585449\n",
            "Iteration 7, loss = 1.27418201\n",
            "Iteration 8, loss = 0.02147437\n",
            "Iteration 9, loss = 0.02447628\n",
            "Iteration 10, loss = 0.00349040\n",
            "Iteration 11, loss = 0.00215598\n",
            "Iteration 12, loss = 0.00150783\n",
            "Iteration 13, loss = 0.00114285\n",
            "Iteration 14, loss = 0.00109285\n",
            "Iteration 15, loss = 0.00113859\n",
            "Iteration 16, loss = 0.00111534\n",
            "Iteration 17, loss = 0.00315483\n",
            "Iteration 18, loss = 0.00141595\n",
            "Iteration 19, loss = 0.00117532\n",
            "Iteration 20, loss = 0.00109590\n",
            "Iteration 21, loss = 0.00108423\n",
            "Iteration 22, loss = 0.00107391\n",
            "Iteration 23, loss = 0.00105034\n",
            "Iteration 24, loss = 0.00091574\n",
            "Iteration 25, loss = 0.00101758\n",
            "Iteration 26, loss = 0.00096676\n",
            "Iteration 27, loss = 0.00091311\n",
            "Iteration 28, loss = 0.00103180\n",
            "Iteration 29, loss = 0.00103035\n",
            "Iteration 30, loss = 0.00102503\n",
            "Iteration 31, loss = 0.00101916\n",
            "Iteration 32, loss = 0.00100886\n",
            "Iteration 33, loss = 0.00099578\n",
            "Iteration 34, loss = 0.00098085\n",
            "Iteration 35, loss = 0.00096003\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-395d31df00fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and continuous targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l3Yu5qOqxZB",
        "colab_type": "code",
        "outputId": "bdc4313d-3445-4f09-b16d-a23f81c35ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.5212726202151294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk3eAD4Zrlag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPiqV7FsjFJ8",
        "colab_type": "code",
        "outputId": "aaf64bb6-c668-45f2-a67d-5445f1f8bc2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "svr= SVR(kernel='rbf', degree=3, gamma='scale', C=1.25, epsilon=0.02, shrinking=True, cache_size=200, verbose=True, max_iter=2000)\n",
        "svr.fit(X_train, y_train)\n",
        "print(\"Training set score: %f\" % svr.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % svr.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]Training set score: -0.399214\n",
            "Test set score: -0.058927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC0xDc-4gJe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp2=MLPRegressor(hidden_layer_sizes=(5,),\n",
        "                                     activation='relu',\n",
        "                                     solver='adam',\n",
        "                                     learning_rate='adaptive',\n",
        "                                     max_iter=1000,\n",
        "                                     learning_rate_init=0.01,\n",
        "                                     alpha=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}